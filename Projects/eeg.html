<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Aaron Pulver - GIS Analyst & Software Developer</title>

    <!-- Bootstrap Core CSS -->
    <link href="../css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom CSS -->
    <link href="../css/grayscale.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">

    <!-- Navigation -->
    <nav class="navbar navbar-custom navbar-fixed-top" role="navigation">
        <div class="container">
            <div class="navbar-header">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-main-collapse">
                    <i class="fa fa-bars"></i>
                </button>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse navbar-right navbar-main-collapse">
                <ul class="nav navbar-nav">
                    <li class="page-scroll">
                        <a href="../index.html">Home</a>
                    </li>
                    <!-- Hidden li included to remove active class from about link when scrolled up past about section -->
                    <!--<li class="page-scroll">
                        <a href="#page-top">Top</a>
                    </li>-->
                    <li>
                        <a class="page-scroll" href="#introduction">Intro</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#data">Data</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#preprocessing">Preprocessing</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#classification">Classification</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#results">Results</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#discussion">Conclusions</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="#references">References</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container -->
    </nav>

    <section id="introduction" class="content-section text-center">
        <div class="section-default gray-section">
            <div class="container">
                <div class="col-lg-8 col-lg-offset-2">
                    <h2>EEG Automatic Volume Control</h2>
                    <h3>A project by Deep Tayal & Aaron Pulver</h3>
                    <h3><a href="eeg/paper.pdf">PDF</a></h3>
                    <p>
                    Understanding the operation of the brain is one the biggest mysteries in life and among the utmost challenges to science. Physically, the human brain consists of about three pounds of grey matter and contains around 10 billion neurons. This seemingly unimpressive organ is responsible for human awareness, emotions, and sensations. Using only thoughts to control the surrounding environment will provide a paralyzed person with a powerful tool which will not rely on peripheral nerves or on emotions. By cautiously studying brain activation patterns, functional maps of the brain have been developed in which the surface of the cortex has been segregated into a set of functionally distinct regions. These different regions are now been studied to determine the working of the brain [1].
                    <p>
                    With the improvement in biomedical technology and machine learning, the analysis of human bio-potential signals have become an important area of research. Among the human bio-potentials, Electroencephalogram (EEG) signals have been widely studied because of their medical importance [2] [3]. EEG signals are electrical action potential signals obtained as a result neurons firing in the brain. 
                    <p>
                    A typical EEG signal, measured from the scalp, will have an amplitude of 10 µV to 100 µV and a frequency in the range of 1 Hz to 100 Hz. The approach is based on earlier observations that the EEG spectrum contains some characteristics waveforms that fall primarily within four frequency bands: delta (<4 Hz), theta (4-8 Hz), alpha (8-13 Hz) and beta (13-30 Hz). 
                    <p>
                    Difficult characteristics of EEG signals such as poor signal to noise ratio (SNR) compared to other bio-potentials give rise to the employment of robust algorithms in order to achieve the goal. Hence, employing efficient classification algorithms has been an important goal and highly attractive area in the research community. [2], [3], [4]
                    <p>
                    Analysis of EEG signals is a three stage process. The first stage is preprocessing the EEG data. Preprocessing is the phase of preparing the data in order to remove the undesired components/noise which may be present as outliners in the data set such as eye blink artifacts. It is usually achieved by filtering the data. This stage is followed by feature extraction process. In this, various pattern recognition and signal processing techniques, e.g. PCA, Band Powers or FFT can be employed in order to extract meaningful features that will be used for classification. The third and the final stage, termed as classification, is to employ a machine learning technique in order to make decisions utilizing the features extracted in the second stage. Our study focuses on designing an EEG based automatic volume control with the application of two different classifiers (SVM & RBFN) in accordance with an efficient optimizing technique (PSO). 
                </div>
            </div>
        </div>
    </section>
    
    <!-- Projects Section -->
    <section id="data" class="content-section text-center">
        <div class="section-default black-section">
            <div class="container">
                <div class="col-lg-8 col-lg-offset-2">
                    <h2> Data Collection </h2>
                    <p>
                    Data was collected using the BioRadio150 and a standard EEG cap based on the international 10-20 system as shown below. Four electrodes were recorded: T3, C3, C4, and T4. Cz was used as the ground. These electrodes were chosen because they are close to the ear and have been shown to be effective for two-class classification [11]. One subject was used for recording the data. He was instructed to choose a song which was approximately 3.5 minutes long. An iPhone 5s was used as the device controlling the music and volume. The user was instructed to set the volume to 10% and to begin listening to the song. Once the song ended, the recording was stopped. This same process was repeated but the volume was changed to 100%. The data was recorded at a rate of 960 Hz. The first and last five seconds of the song was removed to eliminate crescendos and decrescendos. Approximately 30 minutes of both loud and quiet music was recorded. The data was then broken into one second intervals for training, validation, and testing.     
                </div>
                <div class="col-lg-8 col-lg-offset-2">
                    <img src="eeg/system.jpg"/>
                </div>
            </div>
        </div>
    </section>
    
    <!-- Research Section -->
    <section id="preprocessing" class="content-section text-center">
        <div class="section-default gray-section">
            <div class="container">
                <div class="col-lg-8 col-lg-offset-2">
                    <h2>Preprocessing</h2>
                    <p>
                    The mean alpha and beta band powers were chosen as the features which were extracted from each electrode. These are two of the most active frequency bands for brain biosignals [4]. Band powers have been proven to be an efficient method for extracting features for a two-class problem [11]. Taking the band powers requires creating a band-pass filter for the desired range of frequencies, squaring the signal, and then taking the mean. This technique provided a vector of eight features [T3<sub>α</sub> T3<sub>β</sub> C3<sub>α</sub> C3<sub>β</sub> C4<sub>α</sub> C4<sub>β</sub> T4<sub>α</sub> T4<sub>β</sub>].
                </div>
            </div>
        </div>
    </section>
    
    <section id="classification" class="content-section text-center">
        <div class="section-default black-section">
            <div class="container">
                <div class="col-lg-8 col-lg-offset-2">
                    <h2>Classification</h2>
                    <p>
                    Two classification techniques were analyzed: Particle Swarm Optimization with Support Vector Machine (PSO-SVM) and Particle Swarm Optimization with a Radial Basis Function Network (PSO-RBFN).
                    <h3>Particle Swarm Optimization</h3>
                    <p>Particle swarm optimization (PSO) is a population-based stochastic approach for solving continuous and discrete optimization problems. It belongs to the class of swarm intelligence techniques that are used to solve optimization problems. 
                    <p>In particle swarm optimization, simple software agents, called particles, move in the search space of an optimization problem. The position of a particle represents a candidate solution to the optimization problem at hand. Each particle searches for better positions in the search space by changing its velocity according to rules originally inspired by behavioral models of bird flocking [5]. 
                    <img src="eeg/topology.jpg"/>
<p>In PSO, the so-called swarm is composed of a set of particles P={p1,p2,…,pk} . The position of a particle corresponds to a candidate solution of the considered optimization problem, which is represented by an objective function f. At any time step t , pi has a position x⃗ ti and a velocity v⃗ ti associated to it. The best position that particle pi (with respect to f) has ever visited until time step t is represented by vector b⃗ ti (also known as a particle's personal best). Moreover, a particle pi receives information from its neighborhood Ni⊆P . In the standard particle swarm optimization algorithm, the neighborhood relations between particles are commonly represented as a graph G={V,E} , where each vertex in V corresponds to a particle in the swarm and each edge in E establishes a neighbor relation between a pair of particles. The resulting graph is commonly referred to as the swarm's population topology as in “Fig. 2” [6].
<p>The PSO algorithm starts by generating random positions for the particles, within an initialization region Θ′⊆Θ. Velocities are usually initialized within Θ′ but they can also be initialized to zero or to small random values to prevent particles from leaving the search space during the first iterations. During the main loop of the algorithm, the velocities and positions of the particles are iteratively updated until a stopping criterion is met. The update rules are shown in (1) and (2) where where w is a parameter called inertia weight, φ1 and φ2 are two parameters called acceleration coefficients, U⃗ t1 and U⃗ t2 are two n×n diagonal matrices in which the entries in the main diagonal are random numbers uniformly distributed in the interval [0,1) . At each iteration, these matrices are regenerated. Usually, vector l⃗ ti , referred to as the neighborhood best, is the best position ever found by any particle in the neighborhood of particle pi , that is, f(l⃗ ti)≤f(b⃗ tj)∀pj∈Ni . If the values of w, φ1 and φ2 are properly chosen, it is guaranteed that the particles' velocities do not grow to infinity [6]. 
<img src="eeg/eq12.jpg"/>
<p>The three terms in the velocity-update rule above characterize the local behaviors that particles follow. The first term, called the inertia or momentum serves as a memory of the previous flight direction, preventing the particle from drastically changing direction. The second term, called the cognitive component models the tendency of particles to return to previously found best positions. The third term, called the social component quantifies the performance of a particle relative to its neighbors. It represents a group norm or standard that should be attained. 
<p>In some cases, particles can be attracted to regions outside the feasible search space Θ. For this reason, mechanisms for preserving solution feasibility and a proper swarm operation have been devised [7]. One of the least disruptive mechanisms for preserving feasibility is one in which particles going outside Θ are not allowed to improve their personal best position so that they are attracted back to the feasible space in subsequent iterations[8]. 
<p>The PSO used in this paper was tested using a basic two dimensional equation (3). After several iterations, the particles easily converged to X=15 and Y=20. To further test the validity of our PSO, the Rosenbrock equation (4) was used to test for convergence to a global minimum at X=1 and Y=1.
<img src="eeg/eq34.jpg"/>
<p>	<h3>Support Vector Machine</h3>
<p>Support Vector Machine algorithm was originally invented by Vladimir N. Vapnik and the current standard incarnation (soft margin) was proposed by Vapnik and Corinna Cortes in 1995 [9].
<p>SVMs are supervised learning models with associated learning algorithms that analyse data and recognize patterns, used for the purpose of classification and regression analysis. The basic SVM, also termed as non-probabilistic binary linear classifier, takes a set of data and predicts, for each given input, which of the two possible classes forms the output. 
<p>The SVM training, taking into consideration the training data marked to a specific class, builds a model that assign new data into one category or other. The SVM model is a representation of the data as points marked in space, mapped so that the data of the separate categories are divided clearly by a wide gap. In other words, it constructs a hyperplane, or a set of hyperplanes which can be used for the purpose of classification or regression.
<p>The original optimal hyperplane algorithm proposed by Vapnik in 1963 was a linear classifier. However, in 1992, Bernhard E. Boser, Isabelle M. Guyon and Vladimir N. Vapnik suggested a way to create nonlinear classifiers by applying the kernel trick (originally proposed by Aizerman et al.[9]) to maximum-margin hyperplanes [10]. The resulting algorithm is formally similar, except that every dot product is replaced by a nonlinear kernel function. This allows the algorithm to fit the maximum-margin hyperplane in a transformed feature space. The transformation may be nonlinear and the transformed space high dimensional; thus though the classifier is a hyperplane in the high-dimensional feature space, it may be nonlinear in the original input space.
<p>Support vector machines use a nonlinear kernel function to transform the input features into a higher dimension where the data is more separable [12]. The SVM will create optimal hyper-planes with the largest margin between each class. LIBSVM, a software package was used to create the classifier. Different kernel types were tested: linear, polynomial, radial basis function (RBF), and sigmoid. Support vector machines are a popular newer method of classification and been proven to be very efficient both as a standalone classifier and when combined with other techniques such as fuzzy logic [10] [11]. By changing the cost and gamma of the SVM, accuracy can be fine tuned. This was implemented by using PSO to evaluate the accuracy with various combinations until the particles converged to a local maximum which produced the best results.
<img src="eeg/svm.jpg"/>
	<h3>Radial Basis Function Network</h3>

<p>RBFN is a type of feed-forward Neural Network which consists of three layers: input layer, hidden layer, and output layer as shown in the schematic in Figure 4. The input layer contains n dimensional feature vectors entering the network.
The hidden layer is composed of radially symmetric Gaussian kernel functions as shown in “Fig 3.”

<p>In employing RBFN for classification, finding the appropriate centers for kernel functions has critical importance on the generalization capability of the classifier [11]. Therefore, several clustering algorithms are widely used to supervise the cluster centers. In this study, k-means clustering was used to determine the centroids for the hidden layers. The RBFN was modified to have activation functions as shown in (4). Where β is defined in (5) where b is a parameter passed into the RBFN and σ is defined in (6) as the average distance from all the points in a cluster to the center of that cluster.
<img src="eeg/eq456.jpg"/>

<p>The outputs of the hidden layer are connected to the output layer by weighted links.These weights were determined by evaluating the activation values for the RBF neurons for each training sample. These values were then used as inputs to gradient descent to train the weight. 

<p>During the testing phase of the data with RBFN, outputs are found by using the weights obtained in the training phase and activation values calculated using the test data. These outputs are compared to a threshold value or to each other in the case of two classes at the end in order to generate binary class label outputs. The RBFN classifier accepted a number of kernel functions and the b power as parameters. This allowed PSO to optimize the number of hidden layers as well as the power used to calculate the β coefficients. 
<img src = "eeg/rbf.jpg"/>
</div>
            </div>
        </div>
    </section>

    <!-- Contact Section -->
    <section id="results" class="content-section">
        <div class="section-default gray-section">
            <div class="container">
                <div class="col-lg-8 col-lg-offset-2">
                    <h2>Results</h2>
                    <p>
                    After extracting features and using PSO to optimize the parameters to the respective classification methods the best results were recorded in table 1. Standard data sets were used to verify the classification techniques. Then standard BCI data were used to verify preprocessing techniques. Finally, the collected data were preprocessed, optimized, and classified. Cross validation was used for all of these data. This consisted of breaking these data into three sections: learning, validation, and testing. The PSO fitness function tested used the accuracy of both the learning and validation sets to determine the parameters which would give the highest results for these remaining test data. One hundred iterations of PSO were used when optimizing the parameters. The results are comparable to previous work done using the same standard data sets with similar and unique classifiers [4].
                    </div>
                <div class="col-lg-8 col-lg-offset-2">
                <table class="tableizer-table">
                   <thead><tr class="tableizer-firstrow">
                      <th>Data Set</th>
                      <th>RBFN Testing %</th>
                      <th>SVM Testing %</th>
                   </tr></thead>
                   <tr>
                      <td>Breast Cancer</td>
                      <td>99.42<br>C=3<br>b=0.4836</td>    
                      <td>98.83<br>C=9.7938<br>γ=2.33E-05</td>
                    </tr>
                    <tr>
                      <td>Liver Disorder</td>
                      <td>73.26<br>C=53<br>b=4.4452</td>
                      <td>74.418<br>C=73.446<br>y=1.21E-05</td>
                    </tr>
                    <tr>
                      <td>Diabetes Disorder</td>
                      <td>79.17<br>C=40<br>b=2.28417</td>
                      <td>79.17<br>C=2.677<br>y=5.3E-06</td>
                    </tr>
                    <tr>
                      <td>BCI IV 2b</td>
                      <td>69<br>C=63<br>b=0.34641</td>
                      <td>56<br>C=64.472<br>y=0.7262</td>
                    </tr>
                    <tr>
                      <td>Collected Data</td>
                      <td>90.52<br>C=41<br>b=0</td>
                      <td>90.38<br>C=9.96271<br>y=0.7262</td>
                    </tr>
                </table>
                </div>
            </div>
        </div>
    </section>
    
    <!-- Contact Section -->
    <section id="discussion" class="content-section">
        <div class="section-default black-section">
            <div class="container">
                <div class="col-lg-8 col-lg-offset-2">
                    <h2>Discussion & Conclusion</h2>
                    <p>
                    Raw EEG data pertaining to loud and quiet music were successfully collected, processed, and classified with both PSOSVM and PSORBFN. The results show that an SVM with an RBF kernel is comparable to a RBFN itself. Future work includes recording data from more individuals to better train the system, optimizing the MATLAB code to be more time efficient and robust, and to investigate real-time volume control. Currently, these data were recorded to obtain two classes. In the future, this could be expanded to more classes which would correspond to different desired percentages of volume as well as a content state where the user is satisfied with the current volume.
                    </div>
            </div>
        </div>
    </section>
    
    <section id="references" class="content-section">
        <div class="section-default gray-section">
            <div class="container">
                <div class="row">
                    <div class="col-lg-8 col-lg-offset-2">
                        <h2>References</h2>
                        <p>[1]	Society for Neuroscience Brain facts: a primer on the brain and nervous system. Society for Neuroscience, 2008.
                        <p>[2]	F. Lotte, M. Congedo, A. Lecuyer, F. Lamarche and B. Arnaldi , " A review of classification algorithms for eeg-based brain-computer interfaces," Journal of Neural Engineering, vol. 4, pp. RI--RI3, 2007.
                        <p>[3]	S. Sanei and J. Chambers, EEG signal processing, NJ, USA: Wiley Inter-science, 2007.
                        <p>[4]	Cinar, E.; Sahin, F., "A study of recent classification algorithms and a novel approach for EEG data classification," Systems Man and Cybernetics (SMC), 2010 IEEE International Conference on , vol., no., pp.3366,3372, 10-13 Oct. 2010
                        <p>[5]	M. Clerc. Particle Swarm Optimization. ISTE, London, UK, 2006.
                        <p>[6]	Y. Shi and R.C. Eberhart, “Parameter selection in particle swarm optimization”, In Proceedings of the Seventh Annual Conference on Evolutionary Programming, pages 591-600, 1998.
                        <p>[7]	Y. Shi and R.C. Eberhart, “Empirical study of particle swarm optimization”, In Proceedings of the IEEE Congress on Evolutionary Computation (CEC), pages 1945-1950, 1999.
                        <p>[8]	J. Kennedy and R.C. Eberhart, “A discrete binary version of the particle swarm algorithm”, In Proceedings of the World Multiconference on Systemics, Cybernetics and Informatics, pages 4104-4109, 1997.
                        <p>[9]	Aizerman, Mark A.; Braverman, Emmanuel M.; and Rozonoer, Lev I. (1964). "Theoretical foundations of the potential function method in pattern recognition learning". Automation and Remote Control 25: 821–837.
                        <p>[10]	Boser, Bernhard E.; Guyon, Isabelle M.; and Vapnik, Vladimir N.; A training algorithm for optimal margin classifiers. In Haussler, David (editor); 5th Annual ACM Workshop on COLT, pages 144–152, Pittsburgh, PA, 1992. ACM Press
                        <p>[11]	Schwenker, Friedhelm; Kestler, Hans A; Palm, Gunther. “Three learning pahses for radial-basis function networks,” Neural Networks, Volume 14, Issues 4-5, May 2001, Pages 439-458.
                    </div>
                </div>
            </div>
        </div>
    </section>

    <!-- Footer -->
    <footer>
        <div class="container text-center">
            <p>Copyright &copy; Aaron Pulver 2016</p>
        </div>
    </footer>

    <!-- jQuery -->
    <script src="../js/jquery.js"></script>

    <!-- Bootstrap Core JavaScript -->
    <script src="../js/bootstrap.min.js"></script>

    <!-- Plugin JavaScript -->
    <script src="../js/jquery.easing.min.js"></script>

    <!-- Custom Theme JavaScript -->
    <script src="../js/grayscale.js"></script>

</body>

</html>
